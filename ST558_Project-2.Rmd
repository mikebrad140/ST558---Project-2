---
title: 'ST558: Project 2'
author: "Michael Bradshaw and Yejun Han"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_document
params:
  channel:
    label: "Data Channel"
    value: Lifestyle #default value
    input: select # select dropdown
    choices: [Lifestyle, Entertainment, Business, Social Media, Tech, World]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)

library(tidyverse)
library(tree)
library(caret)
library(randomForest)
library(gbm)
library(leaps)
library(MASS)
library(lmtest)
library(doParallel)
```

# Channel of Interest: `r params$channel`

## Introduction to the Project

The dataset used for this analysis focuses on the popularity of online news articles, encompassing approximately 60 variables. These variables include n_tokens_title, n_unique_tokens, num_imgs, num_videos, average_token_length, num_keywords, weekday_is_, is_weekend, rate_positive_words, max_negative_polarity, title_subjectivity, and shares. Our objective is to analyze the data and develop predictive models with the shares variable as the target for each of six different data channels. 

To begin, we imported the news dataset and removed non-predictor variables such as url and timedelta. Next, we conducted summarizations of the data and explored specific variables such as day_of_week, content_length, avg_positive_polarity, num_keywords, and the length of the title. This analysis involved examining relevant summary statistics and generating plots to gain insights.

In the modeling phase, we employed both linear regression models and ensemble tree-based models. These models were applied to predict the number of shares. We then compared the performance of the four created models and selected the optimal model.

## Import the Data
In this section, we first import the raw online news popularity dataset. We remove the non-predictor variables named url and timedelta. Next, we create three new variables: the first is called *channel* based on the values of the *data_channel_is_* variables. The second new variable is called *day_of_week* based on the values of the *weekday_is_* variables. The third new variable is called *content_length* based on the values of the *n_tokens_content* variable. The *content_length *variable is assigned values based on the conditionals provided, categorizing the length of the content as "Very Short," "Short," "Medium," or "Long."

Lastly, we subset the imported newsData into separate data frames for each specific channel.

```{r ImportData, eval=TRUE}
#Import the newsData csv file:
newsData <- read.csv(file="..//OnlineNewsPopularity//OnlineNewsPopularity.csv")

# Create single variable for data channel: 
newsData <- newsData %>% 
  dplyr::select(-url, -timedelta) %>%
  mutate(channel = ifelse(data_channel_is_lifestyle == 1, "Lifestyle",
                   ifelse(data_channel_is_entertainment == 1, "Entertainment",
                   ifelse(data_channel_is_bus == 1, "Business",
                   ifelse(data_channel_is_socmed == 1, "SocialMedia",
                   ifelse(data_channel_is_tech == 1, "Tech",
                   ifelse(data_channel_is_world == 1, "World", "Other")))))),
         day_of_week = ifelse(weekday_is_monday == 1, "Monday",
                       ifelse(weekday_is_tuesday == 1, "Tuesday",
                       ifelse(weekday_is_wednesday == 1, "Wednesday",
                       ifelse(weekday_is_thursday == 1, "Thursday",
                       ifelse(weekday_is_friday == 1, "Friday",
                       ifelse(weekday_is_saturday == 1, "Saturday", "Sunday")))))),
         content_length = ifelse(n_tokens_content <= 250, "Very Short",
                        ifelse(n_tokens_content <= 410, "Short",      
                        ifelse(n_tokens_content <= 750, "Medium", "Long"))),
         title_length = ifelse(n_tokens_title <= 8, "Short",
                               ifelse(n_tokens_title <= 12, "Medium",      
                               ifelse(n_tokens_title <= 15, "Long","Very Long"))),
         avg_positive_polarity_rate = ifelse(avg_positive_polarity <= 0.2, "Low",
                               ifelse(avg_positive_polarity <= 0.3, "Medium",      
                               ifelse(avg_positive_polarity <= 0.4, "High","Very High")))
         )

newsData$channel <- as.factor(newsData$channel) #Converting to factor
newsData$day_of_week <- factor(newsData$day_of_week,
                               levels = c("Sunday", "Monday", "Tuesday", 
                                          "Wednesday", "Thursday", 
                                          "Friday", "Saturday"))
newsData$content_length <- as.factor(newsData$content_length)
# Subset the data for each data channel
newsData_channel <- newsData %>% filter(channel == params$channel)
```

## Splitting the data into test and training datasets

This section creates training and test indices based on the *shares* variable, and splits the data frames for each channel into separate training and test sets for modeling.

```{r SplitData, eval=TRUE}
# Set the seed for reproducibility
set.seed(717)

# Create the training and test indices
trainIndices <- createDataPartition(newsData_channel$shares, p = 0.7, list = FALSE)

# Split the data into training and test sets
train_Data <- newsData_channel[trainIndices, ]
test_Data <- newsData_channel[-trainIndices, ]
```

## Summarizations

In this first example, we look at a table summarizing the statistics of the shares variable in our training dataset. We then create a histogram to visualize the distribution of shares. 

In a normal distribution,the histogram will have a symmetric shape with a peak at the center and tails that extend symmetrically in both directions. In a skewed distribution, the data can be either skewed to the right or skewed to the left. In a right skewed distribution, the histogram will have a long tail on the right side and a shorter tail on the left side. The majority of observations will be concentrated on the left side. In a left skewed distribution, the histogram will have a long tail on the left side and a shorter tail on the right side, with the majority of observations concentrated on the right side.  

```{r Summarization1, eval=TRUE}
summary(train_Data$shares)
# Histogram of shares
ggplot(train_Data , aes(x = shares)) +
  geom_histogram(binwidth = 2500, fill = "blue") +
  labs(x = "Shares", y = "Frequency") +
  ggtitle("Distribution of Shares") +
  theme_classic()
```

In this second table, we display summary statistics for the shares variable within our training dataset by day of the week. We display the count (n), the total sum of shares (total_shares), the average shares (average_shares), the standard deviation of shares (sd_shares), the minimum shares (min_shares), and the maximum shares (max_shares) for each group.

Next, we examine the is_weekend categorical variable by the shares variable. We calculate the mean shares for each group (weekend vs. non-weekend days).

Lastly, we create a bar plot to visualize the average shares by day of the week. Higher bars indicate more shares on that day of the week.

```{r Summarization2, eval=TRUE}
#Shares by day of the week
average_shares <- train_Data %>%
  group_by(day_of_week) %>%
  summarise(n = n(),
            total_shares = sum(shares),
            average_shares = mean(shares),
            sd_shares = sd(shares),
            min_shares = min(shares),
            max_shares = max(shares))
average_shares

weekend_mean_shares <- train_Data %>%
  group_by(is_weekend) %>%
  rename(Weekend = is_weekend) %>%
  mutate(Weekend = ifelse(Weekend == 0, "No", "Yes")) %>%
  summarize(mean_shares = mean(shares))
  
weekend_mean_shares

ggplot(average_shares, aes(x = day_of_week, y = average_shares, 
                           fill = day_of_week)) +
  geom_col() +
  labs(x = "Day of the Week", y = "Average Shares", fill = "Day of the Week") +
  ggtitle("Average Shares by Day of the Week") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

These tables display the average number of shares for different content length categories. Summary statistics such as the count of observations, the average number of shares, the standard deviation of shares, the minimum number of shares (min_shares), and the maximum number of shares (max_shares), and the range of shares (range_shares) are computed for each content length category.

Next, a horizontal bar chart is created to help us visualize these results by showing the average number of shares on the x-axis and the content length categories (content_length) on the y-axis. 

```{r Summarization3, eval=TRUE}
# Calculate the average shares by content length category
average_shares_byContent <- train_Data %>%
  group_by(content_length) %>%
  summarise(n = n(),
            average_shares = mean(shares),
            sd_shares = sd(shares),
            min_shares = min(shares),
            max_shares = max(shares),
            range_shares = max_shares - min_shares)
average_shares_byContent

# Create the horizontal bar chart
ggplot(average_shares_byContent, aes(x = average_shares, y = content_length, fill = content_length)) +
  geom_bar(stat = "identity") +
  labs(x = "Average Shares", y = "Content Length Category", fill = "Content Length") +
  ggtitle("Average Shares by Content Length Category") +
  theme_minimal() +
  theme(axis.text.y = element_text(hjust = 0.5))
```

To analyze the relationship between avg_positive_polarity and average_shares, we transformed the continuous numeric data of avg_positive_polarity into categorical data using the variable avg_positive_polarity_rate. This variable includes the categories "Low," "Medium," "High," and "Very High."

In the plot of average shares as a function of average positive polarity rate, if we observe higher average shares values for categories with higher positivity levels in the average positive polarity rate, such as "High" and "Very High", it suggests a positive relationship between average positive polarity and average shares of the content. On the other hand, if we notice lower average shares for categories with lower positivity levels in the average positive polarity rate, such as "Low" and "Very Low," it could indicate a negative relationship between average positive polarity and average shares of the content.

```{r Summarization4, eval=TRUE}
# Calculate the average shares by avg_positive_polarity_rate category
average_shares_byavg_positive_polarity_rate <- train_Data %>%
  group_by(avg_positive_polarity_rate) %>%
  summarise(n = n(),
            average_shares = mean(shares),
            sd_shares = sd(shares),
            min_shares = min(shares),
            max_shares = max(shares),
            range_shares = max_shares - min_shares)
average_shares_byavg_positive_polarity_rate

# Create the horizontal bar chart
ggplot(average_shares_byavg_positive_polarity_rate, aes(x = avg_positive_polarity_rate, y = average_shares, fill = avg_positive_polarity_rate)) +
  geom_bar(stat = "identity") +
  labs(x = "avg_positive_polarity_rate Category", y = "Average Shares", fill = "avg_positive_polarity_rate") +
  ggtitle("Average Shares by avg_positive_polarity_rate Category") +
  theme_minimal() +
  theme(axis.text.y = element_text(hjust = 0.6))

```

This next graph displays the average number of shares based on the number of keywords, allowing for a comparison of share counts across different keyword categories. In the trend of average shares as a function of the number of keywords, we observe that the shares change with different keywords. If the plot shows an upward trend, this suggests that the article tends to be shared more often when they have a larger number of keywords. Similarly,  if the plot shows a downward trend, this would indicate that articles tends to be shared less with the increase in keywords.

```{r Summarization5, eval=TRUE}
# Calculate the average shares by number of keywords
average_shares_bynum_keywords <- train_Data %>%
  group_by(num_keywords) %>%
  summarise(n = n(),
            average_shares = mean(shares),
            sd_shares = sd(shares),
            min_shares = min(shares),
            max_shares = max(shares),
            range_shares = max_shares - min_shares)
average_shares_bynum_keywords

# Create the horizontal bar chart
ggplot(average_shares_bynum_keywords, aes(x = num_keywords, y = average_shares, fill = num_keywords)) +
  geom_bar(stat = "identity") +
  labs(x = "num_keywords", y = "Average Shares", fill = "num_keywords") +
  ggtitle("Average Shares by number of keywords") +
  theme_minimal() +
  theme(axis.text.y = element_text(hjust = 0.5))
```

In addition to analyzing the effect of content length on average_shares, we also examined the impact of title length on the shares variable. The numeric data of title length was first converted to categorical data of short, medium, long, and very long. From the plot, we can find the relationship between the length of title and the article shares.If the average_share is high with the corresponding title, this would suggest that articles tend to be shared more for that particualr title length.

```{r Summarization6, eval=TRUE}
# Calculate the average shares by title length category
average_shares_byTitle <- train_Data %>%
  group_by(title_length) %>%
  summarise(n = n(),
            average_shares = mean(shares),
            sd_shares = sd(shares),
            min_shares = min(shares),
            max_shares = max(shares),
            range_shares = max_shares - min_shares)
average_shares_byTitle

# Create the horizontal bar chart
ggplot(average_shares_byTitle, aes(x = title_length, y = average_shares, fill = title_length)) +
  geom_bar(stat = "identity") +
  labs(x = "Title Length Category", y = "Average Shares", fill = "Title Length") +
  ggtitle("Average Shares by title Length Category") +
  theme_minimal() +
  theme(axis.text.y = element_text(hjust = 0.5))
```

## Modeling 
### Linear Regression Models

Linear regression is a modeling technique that attempts to form a linear relationship between a response (dependent variable (i.e. shares)) and one or more predictor (independent variables). In simple linear regression, there is one predictor or independent variable as opposed to multiple linear regression where there are multiple predictors. Linear regression seeks to explain the values of our response variable based on our predictor variables by fitting a straight line to the data that minimizes the sum of the squared differences between the observed and predicted values.

Our online news dataset contains a large number of predictors and we don't have a great sense of their relationship to the shares variable. These relationships may change between data channels as well. If we look at correlations between shares and the rest of the possible numeric variables, we see a mix of small positive relationship, small negative relationships, and some with little to no relationship. 

```{r Correlations, eval=TRUE}
# Calculate correlations between shares and all numeric variables
train_Data <- train_Data %>% dplyr::select(-starts_with("data_channel"))

correlations <- cor(train_Data[, sapply(train_Data, is.numeric)], 
                    train_Data$shares)
correlations
```
In general, we see that the number of videos (num_videos), the number of images (num_imgs) and the number of links (num_hrefs) have small positive correlations. We also know from our descriptive analysis that day of the week seems to influence the number of shares as well, but to eliminate redundancy lets remove the weekday_is variables and the is_weekend variables. Other variables that we might expect to influence number of shares include the number of words in the title (n_tokens_title), and the number of words in the content (n_tokens_content). The LDA topic also appears correlated across different data channels, so let's include all of these variables since they probably vary across different data channels. Lastly, in the overall news dataset we notice that average keyword (kw_avg_avg) had a positive correlation with shares, so let's keep this in our model.

We can also remove all the data_channel variables as this information is not needed within each data channel analysis. 

```{r LinearModel1, eval=TRUE}
# subset the data: 
train_Data_model <- dplyr::select(train_Data, shares, num_videos, n_tokens_content, n_tokens_title, num_imgs, num_hrefs, self_reference_min_shares, LDA_00, LDA_01, LDA_02, LDA_03, LDA_04, kw_avg_avg,day_of_week)

# all predictors from our subset:
lm_fit1 <- train(shares ~ . , data = train_Data_model, 
                            method = "lm", preProcess = c("center", "scale"),
                            trControl = trainControl(method = "cv", number = 5))
```

Here, we assess how well our first linear model fits the data based on the ability to accurately predict the shares variable. The RMSE provides an evaluation of the model's performance.

```{r LinearModel1Evaluation, eval=TRUE}
# check the fit of our first linear model:
predslinear1 <- predict(lm_fit1, newdata = test_Data)
# See how well the model fits
postResample(predslinear1, obs = test_Data$shares)
```

Now, let's suppose we were way off with all of our assumptions in terms of the key variables to include. Let's try a different approach and use the leapSeq algorithm. This approach evaluates different subsets of features to determine the subset that produces the best model performance. And in this situation, we can do this with all of the possible predictor variables, not just our subset of key variables. This may require more computational power, so let's do parallel processing. 

```{r LinearModel2, eval=TRUE}
# Parallel Processing
num_cores <- detectCores()-1
cl <- makeCluster(num_cores)
registerDoParallel(cl)

step_model_seq <- train(shares ~ . , data = train_Data, 
                 method = "leapSeq", preProcess = c("center", "scale"),
                 tuneGrid = data.frame(nvmax = 1:10), # up to 10 predictors max
                 trControl = trainControl(method = "cv", number = 5))
step_model_seq$results
step_model_seq$bestTune

# Stop parallel processing
stopCluster(cl)
registerDoSEQ()
```
Here, we assess how well our second linear model fits the data based on the ability to accurately predict the shares variable. The RMSE provides an evaluation of the model's performance.

```{r LinearModel2Evaluation, eval=TRUE}
predslinear2 <- predict(step_model_seq, newdata = test_Data)
# See how well the model fits
postResample(predslinear2, obs = test_Data$shares)
```

### Ensemble Tree-Based Models

### Random Forest Model: 
Random Forest is an ensemble learning technique that uses decision tress to make predictions. The idea is to construct an ensemble of decision trees by training each tree on a random subset of the data and a random subset of the predictors. Each individual tree independently make predictions, but the final prediction is determined by aggregating the results. 

```{r RandomForest, eval=TRUE}
# Parallel Processing
num_cores <- detectCores()-1
cl <- makeCluster(num_cores)
registerDoParallel(cl)

rfFit <- train(shares ~ ., data = train_Data_model, method = "rf",
               trControl = trainControl(method = "cv", number = 5, repeats = 3),
               preProcess = c("center", "scale"),
               tuneGrid = data.frame(mtry = c(1:10)))

# Stop parallel processing
stopCluster(cl)
registerDoSEQ()

#review results:
rfFit$results
rfFit$bestTune
```

Finally, we make predictions on test data using the trained Random Forest model.  The model fit diagnostics compares the predicted values with the observed shares values from the test data. The RMSE provides an evaluation of the model's performance.

```{r RandomForestEvaluation, eval=TRUE}
# Make predictions with random forest model
predsRf <- predict(rfFit, newdata = test_Data)
# Check model fit diagnostics
postResample(predsRf, obs = test_Data$shares)
```
### Boosted tree model: 

Boosted tree model, also known as gradient boosting, combines multiple weak prediction models to create a stronger and more accurate model. Each tree is dependent on prior trees, i.e. fitting the residual of the trees that preceded it, and improve the final accuracy. 

```{r Boostedtree, eval=TRUE}
# Boosted: Define the control parameters for cross-validation  
ctrl <- trainControl(method = "cv", number = 5)

# Define the tuning parameters:
boosted_grid <- expand.grid(n.trees = c(25, 50, 100, 150, 200),
                            interaction.depth = c(1, 2, 3, 4),
                            shrinkage = 0.1,
                            n.minobsinnode = 10)

# Specify the boosted tree model using the gbm package

# Set the number of CPU cores to use
num_cores <- detectCores()-1
# Set up parallel processing
cl <- makeCluster(num_cores)
registerDoParallel(cl)
boostFit <- train(shares ~ ., data = lifestyle_train_Data_model,
                       method = "gbm",
                       trControl = trainControl(method = "repeatedcv", 
                                                number = 5, repeats = 3),
                       tuneGrid = boosted_grid,
                       #suppress output
                       verbose = FALSE )
# Stop parallel processing
stopCluster(cl)
registerDoSEQ()

```

Then, the boosted tree model was evaluated by assessing its performance on the test data.


```{r BoostedTreeEvaluation, eval=TRUE}
# Make predictions with Boosted tree model
predsBf <- predict(boostFit, newdata = test_Data)

# Check model fit diagnostics
postResample(predsBf, obs = test_Data$shares)

```

### Comparison of Models 

In this section, we will compare the performance of our four models: 1) linear regression (subset of predictors), 2) linear regression (all predictors, leap approach)  3) random forest, and 4) boosted tree. We will evaluate the models using the test set with a focus on predictive accuracy measured by the lowest Root Mean Squared Error (RMSE). 

```{r Modelcompare, eval=TRUE}
# Function to determine the best model
find_best <- function(lm1, lm2, rf, boost){
  # Put all the fit results in a data frame
  results <- data.frame(rbind("Linear Model 1"= postResample(lm1, test_Data$shares),
                                  "Linear Model 2"= postResample(lm2, test_Data$shares),
                                  "Random Forest"= postResample(rf, test_Data$shares),
                                  "Boosted Tree" = postResample(boost, test_Data$shares)))

  # Determine the name of the model with the lowest RMSE
  model_winner <- row.names(results)[results$RMSE == min(results$RMSE)]
  # Return both the data frame of results, as well as the row name of best model name in a list
  return(list(results, model_winner))
}

# search through to find the best model
best_model <- find_best(predslinear1, predslinear2, predsRf, predsBf)
# Print out the data frame of RMSE, Rsquared, and MAE
best_model[[1]]
# Print out a message that tells us which model is the best based on lowest RMSE
print(paste("The best model for the", params$channel, "data channel, determined by the lowest RMSE on the test data, is the", best_model[[2]], "model."))
```

